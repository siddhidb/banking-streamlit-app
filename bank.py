# -*- coding: utf-8 -*-
"""Bank.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hv4O7teZ1-_rjREK8IC7byyGR4ssYKxc
"""

# Commented out IPython magic to ensure Python compatibility.
 # importing necessary libraries
 import os
 import numpy as np
 import pandas as pd
 import seaborn as sns
 import matplotlib.pyplot as plt
#  %matplotlib inline
 import warnings
 warnings.filterwarnings("ignore")

"""
 from sklearn.linear_model import LinearRegression
 from sklearn.svm import SVC
 from sklearn.naive_bayes import MultinomialNB
 from sklearn.tree import DecisionTreeClassifier
 from sklearn.neighbors import KNeighborsClassifier
 from sklearn.ensemble import RandomForestClassifier
 from sklearn.ensemble import AdaBoostClassifier
 from sklearn.ensemble import BaggingClassifier
 from sklearn.ensemble import ExtraTreesClassifier
 from sklearn.ensemble import GradientBoostingClassifier
 from xgboost import XGBClassifier
 from sklearn.linear_model import LogisticRegression
 from catboost import CatBoostClassifier
 import lightgbm as lgb
 from lightgbm import LGBMRegressor
 from sklearn.metrics import make_scorer, mean_absolute_error
 from sklearn.metrics import
 """

bank = pd.read_csv('Bank customers.csv')
bank.head()

bank.shape

bank.isnull().sum()

bank.info()

# So we will be needing to extract data from Income_Category columns and other
# categorical columns, since as you can see the data is in string format and␣ the
# range is giving, not an exact value which can be feeded into the model␣directly

sns.set_theme(style='whitegrid')
 sns.boxplot(bank['Customer_Age'])
 plt.show()

# Most of the customers are somewhere near 45 years age, some outlier that are␣ depicted by the
 # dots on the right hand side at the age 70 and maybe 75

bank.columns

bank[['Gender','Credit_Limit']].groupby('Gender').agg(['mean', 'count'])

bank[['Gender','Avg_Utilization_Ratio']].groupby('Gender').agg(['mean','count'])

bank_cards = bank.groupby('Card_Category')
bank_cards['Customer_Age'].max()

bank_cards = bank.groupby('Card_Category')
bank_cards['Customer_Age'].min()

bank_cards['Avg_Utilization_Ratio'].mean()

bank_marital = bank.groupby('Marital_Status')

bank_marital['Card_Category'].value_counts()

# Now lets move forward and see if we have categorical data in our dataset
 bank.head(2)

bank['Attrition_Flag'].value_counts()

bank['Gender'].value_counts()

bank['Education_Level'].value_counts()

bank['Marital_Status'].value_counts()

def ref1(x):
 if x=='M':
  return 1
 else:
  return 0
bank['Gender'] = bank['Gender'].map(ref1)

bank['Gender'].value_counts()

def ref2(x):
 if x =='Existing Customer':
  return 1
 else:
  return 0
bank['Attrition_Flag'] = bank['Attrition_Flag'].map(ref2)

bank['Attrition_Flag'].value_counts()































"""**Updating banking project**"""

# Commented out IPython magic to ensure Python compatibility.
# importing necessary libraries
import os
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
import warnings
warnings.filterwarnings("ignore")

"""
 from sklearn.linear_model import LinearRegression
 from sklearn.svm import SVC
 from sklearn.naive_bayes import MultinomialNB
 from sklearn.tree import DecisionTreeClassifier
 from sklearn.neighbors import KNeighborsClassifier
 from sklearn.ensemble import RandomForestClassifier
 from sklearn.ensemble import AdaBoostClassifier
 from sklearn.ensemble import BaggingClassifier
 from sklearn.ensemble import ExtraTreesClassifier
 from sklearn.ensemble import GradientBoostingClassifier
 from xgboost import XGBClassifier
 from sklearn.linear_model import LogisticRegression
 from catboost import CatBoostClassifier
 import lightgbm as lgb
 from lightgbm import LGBMRegressor
 from sklearn.metrics import make_scorer, mean_absolute_error
 from sklearn.metrics import
 """

bank = pd.read_csv('Bank customers.csv')
 bank.head()

bank.shape

bank.isnull().sum()

bank.info()

# So we will be needing to extract data from Income_Category columns and other
 # categorical columns, since as you can see the data is in string format and␣the
 # range is giving, not an exact value which can be feeded into the model␣directly

sns.set_theme(style='whitegrid')
sns.boxplot(bank['Customer_Age'])
plt.show()



# Most of the customers are somewhere near 45 years age, some outlier that are␣depicted by the
 # dots on the right hand side at the age 70 and maybe 75

bank.columns

bank[['Gender','Credit_Limit']].groupby('Gender').agg(['mean', 'count'])

bank[['Gender','Avg_Utilization_Ratio']].groupby('Gender').agg(['mean','count'])

bank_cards = bank.groupby('Card_Category')
bank_cards['Customer_Age'].max()

bank_cards = bank.groupby('Card_Category')
bank_cards['Customer_Age'].min()

bank_cards['Avg_Utilization_Ratio'].mean()

bank_marital = bank.groupby('Marital_Status')

bank_marital['Card_Category'].value_counts()

# Now lets move forward and see if we have categorical data in our dataset
bank.head(2)

bank['Attrition_Flag'].value_counts()

bank['Gender'].value_counts()

bank['Education_Level'].value_counts()

bank['Marital_Status'].value_counts()

def ref1(x):
 if x=='M':
  return 1
 else:
  return 0
bank['Gender'] = bank['Gender'].map(ref1)

bank['Gender'].value_counts()

def ref2(x):
 if x =='Existing Customer':
  return 1
 else:
  return 0
bank['Attrition_Flag'] = bank['Attrition_Flag'].map(ref2)

bank['Attrition_Flag'].value_counts()

bank.head()

# converting categorical data into numerical data
y = bank['Card_Category']
x = bank.copy()

x.head(2)

x['Income_Category'].value_counts()

from sklearn.preprocessing import LabelEncoder
def label_encoder(feat):
   le = LabelEncoder()
   le.fit(feat)
   print(feat.name, le.classes_)
   return le.transform(feat)

x['Income_Category'] = label_encoder(x['Income_Category'])
x.head()

x['Income_Category'].value_counts()

x['Education_Level'] = label_encoder(x['Education_Level'])
x['Marital_Status'] = label_encoder(x['Marital_Status'])
x.head()

x.info()

x = x.drop(['CLIENTNUM','Card_Category'], axis=1)
 x.shape

y.value_counts()

y = bank[['Card_Category']]
y

y['Card_Category'] = label_encoder(y['Card_Category'])
y.head()

y['Card_Category'].value_counts()

x.head()

x.describe()

#Outlier
def boxplots(col):
  sns.boxplot(x=col, data=x)
  plt.show()

for i in list(x.select_dtypes(exclude=['object']).columns)[0:]:
  # Skip plotting boxplot for 'Attrition_Flag' as it's now binary
  if i != 'Attrition_Flag':
    boxplots(i)

Q1 = x.quantile(0.25)
 Q3 = x.quantile(0.75)
 IQR = Q3- Q1
 UL = Q3 + 1.5 * IQR
 LL = Q1- 1.5 * IQR
 print(UL)
 print()
 print(LL)

((x>UL)| (x <LL)).sum()

x[((x>UL) | (x < LL))] = np.nan

plt.figure(figsize=(16,12))
 sns.heatmap(x.isnull(), cbar=False, cmap='rainbow', yticklabels=False)
 plt.show()

# Applying capping method by using KNN imputation
 from sklearn.impute import KNNImputer
 imputer = KNNImputer(n_neighbors=5)
 x_impute = pd.DataFrame(imputer.fit_transform(x), columns=x.columns)
 x_impute

x_impute.isnull().sum().sum()

# Feature scaling
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
x_scaled = pd.DataFrame(scaler.fit_transform(x_impute), columns=x_impute.columns)

x_scaled

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.2, random_state=42, stratify=y)

# Feature Selection
# Techniques- Correlation analysis, DT feature importance , RF feature importance
# GDboosting feature importance, XGB feature importance , Regularization- ↪ridge, lasso, elasticnet
# VIF- regression , permutation importance, recursive feature technique,  ↪P-Value
# PCA, LDA, T-SNA
# popular- recursive feature technique, pca
# EDA- dtale , dataprep, pandas-profiling, lux, plotly

!pip install dtale

import dtale
dtale.show(bank)

""" 2 RandomForest Classifier

"""

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier()
rf.fit(x_train, y_train)

f_df = pd.DataFrame({"feature":x_train.columns,'rf_importance':rf.feature_importances_}).sort_values(by='rf_importance',ascending=False)

f_df

pred_train_rf = rf.predict(x_train)
 pred_test_rf = rf.predict(x_test)

from sklearn.metrics import confusion_matrix, classification_report,accuracy_score

print("***************Confusion Matrix*********************")
 print(confusion_matrix(y_train, pred_train_rf))
 print("*******************************")
 print(confusion_matrix(y_test, pred_test_rf))

print("***************classification_report*********************")
 print(classification_report(y_train, pred_train_rf))
 print("*******************************")
 print(classification_report(y_test, pred_test_rf))

print("***************accuracy_score*********************")
 print(accuracy_score(y_train,pred_train_rf))
 print("*******************************")
 print(accuracy_score(y_test,pred_test_rf))

"""3 RandomForestClassifierwithfeatureselectionpart-recursive
 featuretechnique
"""

from sklearn.feature_selection import RFE
rf1= RandomForestClassifier()
feature_1 =RFE(estimator=rf1,n_features_to_select=17,step=1)
feature_1 =feature_1.fit(x_train,y_train)

pred_train_rf1= feature_1.predict(x_train)
 pred_test_rf1 =feature_1.predict(x_test)

print("***************accuracy_score*********************")
 print(accuracy_score(y_train,pred_train_rf1))
 print("*******************************")
 print(accuracy_score(y_test,pred_test_rf1))

#cross validationmethod
 from sklearn.model_selection import cross_val_score
 rf_rfe_training= cross_val_score(feature_1, x_train,y_train,cv=10)
 print(rf_rfe_training)
 print()
 print(rf_rfe_training.mean())

""" 4 PCAMethod"""

from sklearn.decomposition import PCA
 pca = PCA(n_components=10)
 pca_fit = pca.fit_transform(x_scaled)

x_train, x_test, y_train, y_test = train_test_split(pca_fit, y, test_size=0.2, random_state=42, stratify=y)

from sklearn.ensemble import GradientBoostingClassifier
gb = GradientBoostingClassifier()
gb.fit(x_train, y_train)

"""
 f_df2 = pd.DataFrame({"feature":x_train.columns,
 'gb_importance':gb.feature_importances_}).
 ↪sort_values(by='gb_importance',
 ↪ascending=False)
 f_df2
 """

pred_train_gb = gb.predict(x_train)
 pred_test_gb = gb.predict(x_test)

print("***************accuracy_score*********************")
 print(accuracy_score(y_train, pred_train_gb))
 print("*******************************")
 print(accuracy_score(y_test, pred_test_gb))

x.columns